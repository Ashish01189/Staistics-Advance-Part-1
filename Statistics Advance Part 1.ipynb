{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "cfc96458-d238-45c5-9518-b327e699d107",
      "cell_type": "markdown",
      "source": "# Statistic Advance Part 1",
      "metadata": {}
    },
    {
      "id": "1bcc4165-c3f5-4809-a265-68e655ff843e",
      "cell_type": "code",
      "source": "# 1. What is a random variable in probability theory?\n    # In probability theory, a random variable is a numerical quantity whose value is determined by the outcome of a random experiment. It allows us to quantify and analyze uncertain outcomes.\n\n# Two Main Types of Random Variables:\n    # 1. Discrete Random Variable\n        # Takes on countable values (e.g., 0, 1, 2, ...).\n        # Example: The number of heads in 3 coin tosses.\n\n    # 2. Continuous Random Variable\n        # Takes on uncountably infinite values (typically within an interval).\n        # Example: The exact height of a randomly selected person.\n# Key Properties:\n    # A random variable is not random in itself once the outcome is known‚Äîit's a function from the sample space to real numbers.\n    # Denoted by uppercase letters (like X,Y), with outcomes written as lowercase (x,y).\n\n    # Example:\n        # Suppose we roll a fair six-sided die.\n        # Let X be the random variable representing the outcome of the roll.\n        # Possible values of X: {1,2,3,4,5,6}\n        # Probability distribution: P(X=x)= 6/1  for each x‚àà{1,2,3,4,5,6}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5a0344b7-48dc-4698-9b8a-9bbe782a9086",
      "cell_type": "code",
      "source": "# 2. What are the types of random variables?\n    # Random variables are categorized based on the type of values they can take. There are two main types of random variables in probability theory:\n\n# 1. Discrete Random Variable\n    # Definition: A random variable that takes on a finite or countably infinite set of distinct values.\n\n# Examples:\n    # Number of heads in 3 coin tosses (values: 0, 1, 2, 3)\n    # Number of customers in a queue\n    # Number on a rolled die (values: 1 through 6)\n# Probability Distribution: Defined by a probability mass function (PMF), which gives the probability of each possible value.\n\n# 2. Continuous Random Variable\n    # Definition: A random variable that takes on an uncountably infinite number of values, typically any value in a range or interval.\n    # Examples:\n        # Height of a person\n        # Time taken to run a marathon\n        # Temperature in a city at noon\n    # Probability Distribution: Defined by a probability density function (PDF). The probability of the variable taking any specific value is zero; instead, we consider probabilities over intervals (e.g., P(2<X<3)).\n\n# Optional: Mixed Random Variables\n    # Definition: A random variable that has both discrete and continuous components.\n    # Example: A system that operates normally for a random continuous time, then fails instantly at a random discrete moment.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "604c28c6-2012-4b51-b555-76a20f35db7e",
      "cell_type": "code",
      "source": "# 3. What is the difference between discrete and continuous distribution?\n    # The difference between discrete and continuous distributions lies in the types of values the random variable can take and how the probabilities are assigned or measured.\n\n#  Discrete Distribution\n    # Applies to: Discrete random variables\n    # Values: Countable (finite or countably infinite)\n    # Probability assignment: Each value has a specific probability\n    # Represented by: Probability Mass Function (PMF)\n    # Total probability: The sum of all individual probabilities is 1\n\n# Example:\n    # Let X be the outcome of a die roll.\n    # Possible values: {1,2,3,4,5,6}\n    # P(X=3)= 1/6\n    # PMF: P(X=x)= 1/6 for x=1,2,...,6\n\n# Continuous Distribution\n    # Applies to: Continuous random variables\n    # Values: Uncountably infinite (usually real intervals)\n    # Probability assignment: Probability is given over an interval\n    # Represented by: Probability Density Function (PDF)\n    # Total probability: The integral of the PDF over all values is 1\n    # Important: P(X=a)=0 for any specific value a; only ranges have nonzero probabilities.\n\n# Example:\n    # Let Y be the time (in seconds) it takes for a web page to load.\n        # Possible values: Any real number >0\n        # PDF might look like: f(y)=Œªe^‚àíŒªy (Exponential distribution)\n        # P(2<Y<3)=‚à´ f(y)dy",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cd66f0e1-023c-4c07-9d97-c83a531868d5",
      "cell_type": "code",
      "source": "# 4. What are probability distribution functions (PDF)?\n    # A Probability Distribution Function (PDF) describes how probabilities are distributed over the values of a random variable. The meaning of PDF depends on whether the variable is discrete or continuous:\n\n# 1. For Discrete Random Variables\n    # The function is called a Probability Mass Function (PMF) (not technically a PDF, but often grouped together as a \"distribution function\").\n    # Definition: A PMF gives the probability that a discrete random variable  X takes a specific value x:\n        # P(X=x)=p(x)\n    # Must satisfy:\n        # 0‚â§p(x)‚â§1\n        # ‚àëx p(x)=1\n\n# Example:\n    # Rolling a fair six-sided die:\n        # p(x)=P(X=x)= 1/6 ,x‚àà{1,2,3,4,5,6}\n\n# 2. For Continuous Random Variables\n    # The function is properly called a Probability Density Function (PDF).\n\n# Definition: A PDF describes the relative likelihood of a continuous random variable X taking a value in a small interval around x.\n    # Probability of a specific value is zero:\n        # P(X=x)=0\n\n# Probability over an interval is given by an integral:\n        # P(a‚â§X‚â§b)=‚à´ f(x)dx\n# Must satisfy:\n    # f(x)‚â•0\n    # ‚à´ ‚àí‚àû ‚àû f(x)dx=1\n\n# Example:\n    # For a standard normal distribution:\n        # f(x)= 1/2œÄ (e^‚àíx2/2)\n    # This bell-shaped curve shows how likely values of X are near the mean.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e66859f5-3775-4a9d-a682-3e9d589ffc53",
      "cell_type": "code",
      "source": "# 5. How do cumulative distribution function (CDF) differ from probability distribution function (PDF)?\n    # The PDF, used for continuous random variables, tells you how densely the probability is packed around each value. It doesn't give you the probability that a variable equals an exact value (because that‚Äôs always zero for continuous variables), but it tells you how likely the variable is to fall near that value.\n    # For example, the normal distribution‚Äôs PDF (the bell curve) peaks at the mean. That means values near the mean are more likely than values far away.\n    # In the discrete case, this role is played by the PMF (Probability Mass Function), which tells you the actual probability at each point. For example, in a die roll, the PMF says the probability of getting a 3 is 1/6.\n\n    # The CDF gives the cumulative probability up to a certain point. In other words, it tells you the probability that the random variable is less than or equal to a given value.\n    # It starts at 0 (as nothing is less than negative infinity), and as you move to the right along the number line, it steadily increases, eventually reaching 1 (because the total probability is 1).\n    # For example, in the standard normal distribution:\n        # The CDF value at 0 is 0.5 ‚Äî meaning there‚Äôs a 50% chance the value is less than or equal to 0.\n        # As you go further right (say to 2), the CDF might be 0.977, meaning there's a 97.7% chance the value is less than or equal to 2.\n\n# The Core Difference\n    # The PDF/PMF shows the instantaneous likelihood (either density or mass), while the CDF shows the accumulated probability up to a point.\n    # Think of the PDF like the speed of a car at any moment ‚Äî how fast it‚Äôs going right now ‚Äî and the CDF like the distance traveled ‚Äî how much ground has been covered up to that point.\n\n# Their Relationship (for continuous variables)\n    # If you have the PDF, you can get the CDF by integrating it. That means you‚Äôre adding up the probability density from the far left up to the value you care about.\n    # Conversely, if you have the CDF, you can get the PDF by differentiating it ‚Äî the PDF is the slope of the CDF curve.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6ee44f21-9455-4518-b30c-ec0468c8ba0e",
      "cell_type": "code",
      "source": "# 6. What is a discrete uniform distribution?\n    # A discrete uniform distribution is one of the simplest and most intuitive probability distributions in statistics. It describes a situation where a finite number of equally likely outcomes exist, and each outcome has the same probability.\n\n    # Definition \n        # A discrete uniform distribution is a probability distribution where a random variable X takes on n distinct values, and each value is equally likely.\n        # Mathematically, if X can take values ùë•1,ùë•2,...,xn, then:\n                # P(X=xi)= 1/n for¬†all¬†i=1,2,...,n\n\n# Example: Rolling a Fair Die\n    # Let ùëã represent the outcome of rolling a fair six-sided die. The possible values are:\n            # X‚àà{1,2,3,4,5,6}\n    # Since the die is fair, each outcome has the same probability:\n            # P(X=x)= 1/6 for¬†each¬†x\n    # This is a discrete uniform distribution over the integers from 1 to 6.\n\n# Properties\n    # 1. Equal probability: Each outcome is equally likely.\n    # 2. Finite set: Only a limited number of values.\n    # 3. Mean (Expected value): E[X]= (a+b)/2\n         # where a and b are the minimum and maximum values.\n    # 4. Variance: Var(X)= ((b‚àía+1)^2 ‚àí1)/12\n\n# Use Cases\n    # Rolling fair dice\n    # Drawing a random card from a well-shuffled deck\n    # Random selection from a list of options (e.g. choosing a winner from a group)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "03040ef3-c2bc-49c7-a18b-7950ac9ac8f9",
      "cell_type": "code",
      "source": "# 7. What are the key properties of Bernouli distribution?\n    # The Bernoulli distribution is the simplest and most fundamental probability distribution in statistics. It models a random experiment with exactly two possible outcomes: success (usually coded as 1) and failure (coded as 0).\n\n# Key Properties of the Bernoulli Distribution\n    # 1. Two Outcomes Only\n        # The random variable X can only take two values: \n                # X‚àà{0,1}\n        # X=1 represents success, and X=0 represents failure.\n\n    # 2. Probability Mass Function (PMF)\n        # The probability of success is p, and failure is 1‚àíp. The PMF is:\n                # P(X=x)={p (1‚àíp) if¬†x=1 if¬†x=0\n                # Or more compactly: P(X=x)=p^x (1‚àíp)^1‚àíx,x‚àà{0,1}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2ab1975c-760d-40c2-a7d9-14226ded84a9",
      "cell_type": "code",
      "source": "# 8. What is the binomial distribution, and how is it used in probability?\n    # The binomial distribution is a discrete probability distribution that models the number of successes in a fixed number of independent trials, where each trial has exactly two possible outcomes (usually called ‚Äúsuccess‚Äù and ‚Äúfailure‚Äù).\n\n# Definition\n    # A random variable X follows a binomial distribution if:\n        # You perform n independent trials.\n        # Each trial has a success probability of p.\n        # The variable X counts how many successes occur in those n trials.\n    # We write this as:X‚àºBinomial(n,p)\n\n#  Probability Mass Function (PMF)\n    # The probability of getting exactly k successes in n trials is given by:\n            # P(X=k)=(n/k)p^k(1‚àíp)^(n‚àík)\n\n# Key Properties\n    # Mean (Expected value):\n            # E[X]=np\n    # Variance:\n            # Var(X)=np(1‚àíp)\n    # Support:\n            # X‚àà{0,1,2,...,n}\n\n#  When Is It Used?\n    # The binomial distribution is used when:\n        # You repeat the same experiment n times.\n        # Each trial is independent.\n        # Each trial has only two outcomes (success/failure).\n        # The probability of success remains constant across trials.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "aac9b406-1ae6-4086-9bf7-2abd96b4de08",
      "cell_type": "code",
      "source": "# 9. What is the poisson distribution and where is it applied?\n    # The Poisson distribution is a discrete probability distribution that models the number of times an event occurs in a fixed interval of time or space, given a constant average rate and the assumption that the events occur independently.\n\n# Definition\n    # Let X be a random variable representing the number of occurrences of an event in a fixed interval. Then X follows a Poisson distribution with parameter Œª (lambda), where:\n        # Œª>0 is the average number of events in the interval.\n        # The probability mass function is:\n                # P(X=k)= (Œª^k e^‚àíŒª)/k! ,k=0,1,2,...\n\n# Key Properties\n    # Mean (Expected value):\n            # E[X]=Œª\n    # Variance:\n            # Var(X)=Œª\n    # Support:\n            # X‚àà{0,1,2,3,‚Ä¶}\n\n    # The events must be:\n        # Independent: One event doesn't affect another.\n        # Rare: Each event has a low probability, but many opportunities for it to happen.\n\n# Typical Applications\n    # The Poisson distribution is used when you're counting the number of times an event occurs over a certain unit (time, space, volume, etc.), especially when events occur randomly but at a steady average rate.\n    # Examples:\n        # 1. Call center: Number of calls received per minute.\n        # 2. Traffic: Number of cars passing through a checkpoint in an hour.\n        # 3. Biology: Number of mutations in a strand of DNA over a given length.\n        # 4. Finance: Number of defaults or rare events over a fixed period.\n        # 5. Website: Number of hits on a page per second.\n        # 6. Manufacturing: Number of defects on a sheet of metal or product surface.\n\n# Relation to Other Distributions\n    # The Poisson distribution can be seen as a limit of the binomial distribution when:\n        # The number of trials n‚Üí‚àû\n        # The probability of success p‚Üí0\n        # But np=Œª remains constant\n    # This makes it especially useful for modeling rare events over large populations or periods.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "663c35da-668f-44ff-9341-0f719d8c7a53",
      "cell_type": "code",
      "source": "# 10. What is a continuous uniform distribution?\n    # The continuous uniform distribution is a probability distribution that describes a situation where all values within a certain interval are equally likely to occur. It's the continuous counterpart to the discrete uniform distribution.\n\n# Definition\n    # A random variable X is said to have a continuous uniform distribution on the interval [a,b] if the probability is uniformly spread across this interval. That means every value between a and b is equally likely, and no values outside this interval can occur.\n    # We write this as:\n        # X‚àºUniform(a,b)\n\n# Probability Density Function (PDF)\n    # f(x)={ (1/b‚àía) 0 for ùëé‚â§ùë•‚â§ùëè   otherwise\n    # This means the height of the distribution is constant between a and b.\n\n#  Key Properties\n    # Mean (Expected value):\n        # E[X]= (a+b)/2\n\n    # Variance:\n        # Var(X)= ((b‚àía)^2)/2\n \n    # The graph of the PDF is a horizontal line from a to b.\n\n# Example\n    # Suppose a bus arrives at a stop every 20 minutes. If you arrive at the stop at a random time, and you're equally likely to arrive at any point within that 20-minute window, then the waiting time X is uniformly distributed between 0 and 20 minutes:\n            # X‚àºUniform(0,20)\n\n#  When to Use It\n    # Use the continuous uniform distribution when:\n    # You have no reason to believe that some values in an interval are more likely than others.\n    # Events are uniformly random within a continuous range.\n\n# Common applications include:\n    # Modeling waiting times for randomly timed events.\n    # Random number generation (e.g., using a computer to pick a number between 0 and 1).\n    # Simulating uniform uncertainty over an interval.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d9e90d22-6f21-4e85-981b-7546803f1d69",
      "cell_type": "code",
      "source": "# 11. What are the characteristics of a normal distribution?\n    # The normal distribution (also known as the Gaussian distribution) is one of the most important and widely used probability distributions in statistics. It describes many natural phenomena and forms the basis of many statistical methods and tests.\n\n# Key Characteristics of a Normal Distribution\n    # 1. Bell-Shaped Curve\n        # The graph of the normal distribution is a symmetric, smooth, bell-shaped curve.\n        # It is highest at the mean and decreases symmetrically on both sides.\n\n    # 2. Symmetry About the Mean\n        # The distribution is perfectly symmetric around the mean (Œº).\n        # This means that the left side is a mirror image of the right side.\n    # 3. Mean = Median = Mode\n        # In a normal distribution, the mean, median, and mode are all equal and located at the center of the distribution.\n\n    # 4. Defined by Two Parameters\n        # The normal distribution is completely described by:\n        # Mean (Œº): the center of the distribution.\n        # Standard deviation (œÉ): the spread or width of the distribution.\n\n    # 5. Empirical Rule (68-95-99.7 Rule)\n        # For a normal distribution:\n        # About 68% of data falls within 1 standard deviation of the mean.\n        # About 95% falls within 2 standard deviations.\n        # About 99.7% falls within 3 standard deviations.\n    # This rule helps assess how unusual a value is.\n\n    # 6. Probability Density Function (PDF)\n        # The PDF of a normal distribution is given by:\n            # f(x)= (1/root 2œÄœÉ^2) e^2((x‚àíŒº)/2)\n \n        # This function defines the \"bell\" shape mathematically.\n\n    # 7. Asymptotic Behavior\n        # The tails of the curve never touch the horizontal axis; they extend infinitely in both directions.\n        # So theoretically, any value is possible, although extremely unlikely if far from the mean.\n\n    # 8. Area Under the Curve = 1\n        # The total probability under the curve is 1.\n        # The area under the curve between two points gives the probability of the variable falling in that interval.\n\n    # 9. Standard Normal Distribution\n        # A special case of the normal distribution where:\n            # Mean Œº=0\n        # Standard deviation œÉ=1\n\n        # Often used in z-score calculations:\n            # Z= (X‚àíŒº)/œÉ\n\n# Applications\n    # Heights, weights, test scores, measurement errors\n    # Statistical inference (e.g., confidence intervals, hypothesis tests)\n    # Central Limit Theorem: sums/averages of large samples tend to be normally distributed\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ed1be426-4860-42b7-8515-903e64fe7ff4",
      "cell_type": "code",
      "source": "# 12. What is the standard normal distribution, and why is it important?\n    # The standard normal distribution is a special case of the normal distribution that has:\n        # A mean of 0\n        # A standard deviation of 1\n\n    # It is denoted as:\n        # Z‚àºN(0,1)\n    # The random variable for the standard normal distribution is typically written as Z, and values drawn from it are called z-scores.\n\n# Why Is It Important?\n    # 1. Simplifies Calculations\n        # The standard normal distribution provides a common reference for all normal distributions. Any normal distribution X‚àºN(Œº,œÉ^2) can be converted to the standard normal using the z-score formula:\n                # Z= (X‚àíŒº)/œÉ\n        \n        # This standardization allows you to:\n        # 1. Compare scores from different normal distributions\n        # 2. Use standard normal tables to find probabilities\n        # 3. Perform statistical inference\n\n    # 2. Used in Statistical Tests\n        # 1. It is foundational in z-tests, confidence intervals, and hypothesis testing.\n        # 2. Many inferential statistics (e.g. test statistics, control charts) rely on standard normal values.\n\n    # 3. Central Limit Theorem (CLT)\n        # According to the CLT, the sampling distribution of the sample mean approaches a normal distribution (and can be standardized to a standard normal) as the sample size increases ‚Äî even if the original data are not normally distributed.\n\n    # 4. Probability Lookup\n        # Because the standard normal has fixed values, we can use z-tables or software to find exact probabilities. For example:\n                #1.  P(Z<1.96)‚âà0.975\n                #2.  P(‚à£Z‚à£<1.96)‚âà0.95 ‚Üí used in 95% confidence intervals\n\n    # Summary of Properties\n        # Mean: 0\n        # Standard deviation: 1\n        # Symmetrical, bell-shaped\n        # Total area under the curve = 1\n        # Used to standardize any normal variable\n\n    #  Example\n        # Suppose exam scores are normally distributed with a mean of 70 and standard deviation of 10. A score of 85 corresponds to:\n                # Z= (85‚àí70)/10 =1.5\n        # Using the standard normal distribution, we can find the probability of scoring below 85.\n        # Let me know if you'd like help using z-tables or calculating probabilities with z-scores!",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9c79072f-cc0e-4852-b923-7f0a851fdcaf",
      "cell_type": "code",
      "source": "# 13. What is the Central Limit Theorem (CLT), and why is it critical in statistics?\n    # The Central Limit Theorem (CLT) is one of the most important results in statistics because it provides the foundation for making inferences about population parameters from sample statistics.\n    # When you take sufficiently large random samples from any population (with a finite mean and variance), the distribution of the sample means will approach a normal distribution, regardless of the shape of the original population distribution.\n\n# Key Components:\n    # 1. Random sampling: The samples must be independent and randomly selected.\n    # 2. Sample size: Larger sample sizes (usually n‚â•30) make the approximation better.\n    # 3. Finite variance: The population must have a finite mean (Œº) and variance (œÉ¬≤).\n    # 4. Result: The distribution of the sample mean X tends toward a normal distribution:\n            X‚àºN(Œº, (œÉ^2)/n)\n\n# Why is the CLT Critical in Statistics?\n    # 1. Enables Statistical Inference:\n        # It allows us to use the normal distribution to estimate probabilities and create confidence intervals or hypothesis tests even when the population distribution is unknown.\n\n    # 2. Supports the Use of Parametric Methods:\n        # Many statistical tests (like the t-test, z-test, ANOVA) rely on the assumption of normality. The CLT justifies this assumption for sample means.\n\n    # 3. Predictability and Practicality:\n        # In real-world scenarios, we rarely know the true population distribution. The CLT assures us that we can still make accurate estimates using the sample mean.\n\n    # 4. Foundation for Many Models:\n        # It's the theoretical backbone of regression analysis, control charts, Bayesian methods, and more.\n\n# Example:\n    # Imagine measuring the heights of adult men. Even if the true population of heights is slightly skewed, if you take multiple random samples (e.g., of size 50) and plot the means of those samples, that distribution of sample means will form a bell-shaped curve.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "faccd3e3-5a20-487b-a7c2-705ffb954e41",
      "cell_type": "code",
      "source": "# 14. What is the Central Limit Theorem relate to the normal distribution?\n    # The Central Limit Theorem (CLT) is a fundamental concept in statistics that explains how the normal distribution emerges from the behavior of averages (or sums) of random variables.\n\n# Here's how the Central Limit Theorem relates to the normal distribution:\n    # The CLT states that, regardless of the original distribution of the data, the sampling distribution of the sample mean (or sum) will approximate a normal distribution as the sample size becomes large.\n\n# More formally: Let X1,X2,‚Ä¶,Xn be a set of independent and identically distributed (i.i.d.) random variables with:\n    # Mean Œº\n    # Finite variance œÉ^2\n# Then the standardized sample mean:\n        # Z= (X‚àíŒº)/œÉ/root n\n# converges in distribution to a standard normal distribution N(0,1) as n‚Üí‚àû.\n\n# Key Points:\n    # Original distribution doesn't have to be normal. The data can be skewed, uniform, exponential, etc.\n    # Sample size matters. The approximation improves with larger n. A common rule of thumb is n‚â•30, though fewer may be sufficient if the original distribution is not too skewed.\n    # Normal distribution arises naturally. This is why the normal distribution is so common in statistics and probability, especially in inferential statistics like confidence intervals and hypothesis tests.\n\n# Why it matters:\n    # It justifies using normal distribution-based methods even when the population distribution isn't normal, as long as you're working with sample means or sums and your sample size is large enough.\n    # It explains why the bell curve appears so frequently in real-world data ‚Äî many processes are the result of many small, random influences that \"average out.\"",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6defb0d8-e014-43f7-b98e-9762ad19e16d",
      "cell_type": "code",
      "source": "# 15. What is the application of Z-score, and what does it represent?\n    # The Z-score is a statistical measure that tells you how many standard deviations a data point is from the mean. It's widely used in many fields, especially in data analysis, quality control, psychology, and finance.\n\n# The formula is:\n    # Z=(X‚àíŒº)/œÉ\n\n# Where:\n    # X = raw data value\n    # Œº = mean of the population\n    # œÉ = standard deviation\n\n# Interpretation:\n    # Z=0: The value is exactly at the mean.\n    # Z=+1: The value is 1 standard deviation above the mean.\n    # Z=‚àí2: The value is 2 standard deviations below the mean.\n    # The higher the absolute value, the more \"unusual\" the data point is.\n\n# Applications of Z-scores:\n    # 1. Comparing Different Distributions\n        # Z-scores let you compare values from different scales or different distributions.\n        # Example: A student scores 85 in math and 78 in English. If the math class average is 80 (SD = 5) and English average is 70 (SD = 4), you can calculate Z-scores to see which score is better relative to its group.\n\n    # 2. Detecting Outliers\n        # Typically, data points with Z-scores greater than +3 or less than -3 are considered outliers.\n\n    # 3. Probability and Percentiles\n        # In a normal distribution, Z-scores correspond to probabilities.\n        # Z=1.96 covers 95% of the data.\n        # This is essential for confidence intervals and hypothesis testing.\n\n    # 4. Standardizing Data\n        # Z-scores are used to normalize data (mean = 0, SD = 1), which is useful in:\n            # Machine learning\n            # Principal component analysis (PCA)\n            # Clustering algorithms\n\n    # 5. Grading and Assessment\n        # In standardized testing (like SAT, IQ scores), Z-scores help convert raw scores into a standard scale.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9bbe39f5-461c-41d8-b4c7-bc8d5ab2e66f",
      "cell_type": "code",
      "source": "# 16. How do you calculate a Z-score, and what does it represent?\n\n    # The Z-score is calculated using the following formula:\n            # Z=(X‚àíŒº)/œÉ\n    \n    # Where:\n        # X = the individual data point\n        # Œº = the mean of the population (or sample)\n        # œÉ = the standard deviation of the population (or sample)\n\n    # What Does the Z-score Represent?\n        # A Z-score tells you how many standard deviations a data point X is from the mean Œº.\n            # Positive Z-score: The data point is above the mean\n            # Negative Z-score: The data point is below the mean\n            # Z = 0: The data point is exactly at the mean\n\n    # Interpretation Example\n        # Suppose:\n            # X=90\n            # Œº=80\n            # œÉ=5\n        # Then:\n            # Z= (90‚àí80)/5  = 10/5  =2\n\n# This means the score of 90 is 2 standard deviations above the mean.\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f104d787-65c2-4b50-a5d9-934b077ee4f9",
      "cell_type": "code",
      "source": "# 17. What are point estimates and interval estimates in statistics?\n    # These are two key ways to estimate unknown population parameters (like the population mean or proportion) using sample data.\n\n#  1. Point Estimate\n    # Definition: A point estimate is a single value that serves as a best guess for an unknown population parameter.\n    # Examples:\n        # The sample mean x is a point estimate of the population mean Œº.\n        # The sample proportion p^ is a point estimate of the population proportion p.\n\n    # Limitation:\n        # It doesn't give any information about uncertainty or confidence in the estimate.\n\n#  2. Interval Estimate\n    # Definition:\n        # An interval estimate provides a range of values (an interval) that is likely to contain the unknown population parameter, along with a confidence level.\n\n    # Common Form:\n        # Point¬†Estimate ¬± Margin¬†of¬†Error\n        # This forms a confidence interval (CI).\n\n    # Interpretation Example:\n        # If you calculate a 95% confidence interval for the mean income to be $45,000‚Äì$55,000, you are saying:\n\n# Why They're Important:\n    # Point estimates give you a starting value.\n    # Interval estimates let you understand how reliable that estimate is, which is crucial in scientific research, polling, quality control, and business forecasting.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4529a83a-6a03-4a66-b985-53690c66df21",
      "cell_type": "code",
      "source": "# 18. What is the significance of confidence intervals in statistical analysis?\n    # A confidence interval (CI) is one of the most powerful tools in statistics. It provides a range of plausible values for an unknown population parameter and expresses the uncertainty around a point estimate.\n\n    # What is a Confidence Interval?\n        # A confidence interval is a range of values, derived from sample data, that is likely to contain the true population parameter (like the mean or proportion) with a specified level of confidence (e.g., 95%, 99%).\n        # Confidence¬†Interval = Point¬†Estimate ¬± Margin¬†of¬†Error\n\n    # Why Confidence Intervals Matter\n        # 1. Quantifies Uncertainty\n            # Instead of just stating a point estimate (e.g., the average income is $50,000), a CI tells us how precise that estimate is.\n            # Example: ‚ÄúWe are 95% confident that the average income is between $48,000 and $52,000.‚Äù\n\n        # 2. Helps in Decision Making\n            # In business, medicine, or public policy, knowing how much error is possible in an estimate can guide more informed decisions.\n\n        # 3. Provides Statistical Significance Context\n            # If a confidence interval does not include a null value (e.g., 0 in difference of means), it suggests statistical significance.\n            # Example: A 95% CI for a drug's effect is (2.1, 4.3), which doesn‚Äôt include 0, so the effect is likely real.\n\n        # 4. Preferred Over P-values Alone\n            # While p-values only tell you if something is statistically significant, confidence intervals show the range of possible effects, which is more informative.\n\n        # 5. Used in Almost Every Statistical Test\n            # Confidence intervals appear in:\n            # Estimating means, proportions\n            # Regression coefficients\n            # Hypothesis tests\n            # Risk assessments\n\n# Confidence Level Meaning\n    # A 95% confidence level means that if you repeated the sampling many times, about 95% of the calculated intervals would contain the true population parameter.\n    # Important: It does not mean there's a 95% probability the true value is in one specific interval (the value is fixed; the interval is random).",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b47d5f2f-7f88-44d0-815a-46153fe63607",
      "cell_type": "code",
      "source": "# 19. What is the relationship between a Z-score and a confidence interval?\n    # The Z-score and confidence interval are directly connected in statistical analysis ‚Äî the Z-score determines the margin of error used to create a confidence interval when the population standard deviation is known (or the sample size is large).\n\n# Key Relationship:\n    # The Z-score (critical value) defines how many standard deviations you extend from the sample mean to build the confidence interval.\n    # Confidence Interval Formula (for population mean with known œÉ):\n            # CI=XÀâ¬±Z^‚àó‚ãÖ (œÉ)/root n\n\n    # Where:\n        # XÀâ = sample mean\n        # œÉ = population standard deviation\n        # n = sample size\n        # Z‚àó = Z-score (critical value) for your desired confidence level\n\n# Example:\n    # Suppose:\n        # ùëãÀâ= 100\n        # œÉ=15\n        # n=36\n        # 95% confidence level ‚Üí Z‚àó =1.96\n\n    # Then:\n        # Margin¬†of¬†Error =1.96‚ãÖ 15/root36 =1.96‚ãÖ2.5 = 4.9\n        # CI=100¬±4.9=(95.1,¬†104.9)\n    # This means we're 95% confident the true population mean lies between 95.1 and 104.9.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9ad478f5-5d1d-499f-8a74-86280155bd2b",
      "cell_type": "code",
      "source": "# 20. How are Z-scores used to compare different distributions?\n    # Z-scores are used to compare different distributions by standardizing values from different datasets, making them directly comparable even if the original distributions have different means and standard deviations. Here's how that works:\n    # A Z-score tells you how many standard deviations a particular data point is from the mean of its distribution. It is calculated using the formula:\n            # Z=(X‚àíŒº)/œÉ\n\n    # Where:\n        # X is the raw score,\n        # Œº is the mean of the distribution,\n        # œÉ is the standard deviation of the distribution.\n\n# How Z-scores help compare different distributions:\n    # 1. Standardization: By converting values to Z-scores, you're transforming different distributions into a standard normal distribution (mean = 0, standard deviation = 1). This allows for meaningful comparisons.\n    # 2. Relative Positioning: A Z-score shows where a value stands relative to its own distribution. For example, a Z-score of +2 means the value is 2 standard deviations above the mean‚Äîregardless of what the original scale was.\n    # 3. Cross-context Comparisons:\n        # Suppose Student A scores 85 on a math test (mean = 70, SD = 10), and Student B scores 78 on a science test (mean = 65, SD = 8).\n        # Student A‚Äôs Z = (85‚àí70)/10 = 1.5\n        # Student B‚Äôs Z = (78‚àí65)/8 = 1.625\n        # Even though Student A scored higher in absolute terms, Student B performed better relative to their group.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "131a5d0f-7d91-4f29-abc5-c40d9c82d0b9",
      "cell_type": "code",
      "source": "# 21. What are the assumptions for applying the Central Limit Theorem?\n    # The Central Limit Theorem (CLT) is a fundamental concept in statistics that states:\n        # The sampling distribution of the sample mean approaches a normal distribution as the sample size increases, regardless of the population‚Äôs original distribution, given certain assumptions are met.\n\n# Key Assumptions for Applying the Central Limit Theorem:\n    # 1. Independence of observations\n            # Each sample must be drawn independently from the population.\n            # This means the outcome of one observation should not influence another.\n            # In practice: random sampling or random assignment usually ensures independence.\n\n    # 2. Identically distributed samples\n            # The samples should come from the same population and have the same probability distribution.\n            # This ensures consistency in mean and variance across the sample.\n\n    # 3. Sample size is sufficiently large\n            # The larger the sample size, the more the sampling distribution of the mean approximates a normal distribution.\n            # A rule of thumb:\n                # If the population is normal, even small samples (e.g., n<30) will work.\n                # If the population is not normal or skewed, a larger sample size (e.g.,n‚â•30 or more) is needed.\n                # If there are extreme outliers or heavy tails, even larger samples may be necessary.\n\n    # 4. Finite variance\n            # The population should have a finite mean and variance.\n            # If the variance is infinite or undefined (e.g., in heavy-tailed distributions like Cauchy), the CLT doesn't apply.\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "aaf94542-4cc9-4e65-8651-9a841be86401",
      "cell_type": "code",
      "source": "# 22.What is the concept of expected value in a probability distribution?\n    # The expected value (also known as the mean or mathematical expectation) of a probability distribution is a measure of the central tendency ‚Äî it gives you the long-run average outcome if an experiment or process were repeated many times.\n    # For a discrete random variable X with possible values x1,x2,...,xn and corresponding probabilities P(X=xi), the expected value E[X] is:\n                # E[X]=‚àëxi‚ãÖP(X=xi)\n    #  Interpretation\n        # Think of expected value as a weighted average, where each outcome is weighted by its probability.\n        # It does not have to be a value the random variable can actually take. It just represents the average over the long run.\n\n    # Example (Discrete)\n        # A fair 6-sided die has outcomes 1,2,3,4,5,6, each with probability 1/6. Then:\n                    #E[X]= 6‚àëi=1 i‚ãÖ 1/6 = (1+2+3+4+5+6)/6 =3.5\n        # So, the expected value of a fair die roll is 3.5 ‚Äî even though you can never roll a 3.5.\n\n    #  Importance\n        # It helps in decision making under uncertainty.\n        # It's used in economics, insurance, gambling, machine learning, and more.\n        # Forms the basis for other concepts like variance, standard deviation, and utility.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "18ae5c11-d5ea-4ce5-bc5b-1b462f75f0fc",
      "cell_type": "code",
      "source": "# 23. How does a probaility distrution relate to the expected outcome of a random variable?\n    # A probability distribution and the expected outcome (expected value) of a random variable are tightly connected ‚Äî the probability distribution defines how likely each outcome is, and the expected value summarizes those outcomes into a single average based on their likelihood.\n    # A random variable can take on different values, and a probability distribution assigns a probability to each possible value.\n    # The expected value is essentially the weighted average of all possible values of the random variable, where the weights are the probabilities from the distribution.\n\n    # Key Idea:\n        # The expected value is determined entirely by the probability distribution.\n\n    # 1. For a Discrete Random Variable: \n        # Let X be a random variable with outcomes x1,x2,...,x and probabilities P(X=xi).\n            # E[X]= n‚àëi=1‚ãÖP(X=x i)\n        # This formula shows how the expected value depends on:\n            # The possible values of the random variable\n            # The probability of each value (from the distribution)\n\n    # 2. For a Continuous Random Variable:\n            # With probability density function f(x), the expected value is:\n                    # E[X]=‚àû‚à´‚àí‚àû x‚ãÖf(x)dx\n            # Again, the PDF (the distribution) dictates how values contribute to the average.\n\n    # Example (Connection in Practice)\n        # Say you're playing a game where:\n            # You win $10 with probability 0.2\n            # You win $5 with probability 0.5\n            # You lose $2 with probability 0.3\n    # Let X be your net gain. The probability distribution of X is:\n            # P(X=10)=0.2\n            # P(X=5)=0.5\n            # P(X=‚àí2)=0.3\n    # The expected outcome is:\n            # E[X]=(10)(0.2)+(5)(0.5)+(‚àí2)(0.3)=2+2.5‚àí0.6=3.9\n    # So on average, you'd expect to gain $3.90 per game in the long run ‚Äî this is derived entirely from the probability distribution.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8df157f5-0fca-40f4-8b6b-776af33c843c",
      "cell_type": "markdown",
      "source": "# Practical",
      "metadata": {}
    },
    {
      "id": "e8d8e26a-3dff-4237-ba3b-169abe467258",
      "cell_type": "code",
      "source": "# 1. Write a Python program to generate a random variable and display its value.\n\nimport random\n\nrandom_variable = random.randint(1, 6)\n\nprint(f\"The value of the random variable is: {random_variable}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1857eeb1-0d3a-473d-b91b-025933749d6d",
      "cell_type": "code",
      "source": "# 2. Generate a discrete uniform distribution using Python and plot the probability mass function (PMF).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import randint\nlow = 1\nhigh = 7  \n\ndist = randint(low, high)\n\nx = np.arange(low, high)\npmf = dist.pmf(x)\n\nplt.figure(figsize=(8, 5))\nplt.stem(x, pmf, basefmt=\" \", use_line_collection=True)\nplt.xlabel('Value')\nplt.ylabel('Probability')\nplt.title('PMF of Discrete Uniform Distribution (1 to 6)')\nplt.grid(True)\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "45028c76-e7d7-4b5e-8ba5-de3e5bd23148",
      "cell_type": "code",
      "source": "# 3. Write a Python function to calculate the probability distribution function (PDF) of a Bernoulli distribution. \n\ndef bernoulli_pdf(x, p):\n    if not (0 <= p <= 1):\n        raise ValueError(\"Probability p must be between 0 and 1.\")\n    if x not in (0, 1):\n        raise ValueError(\"x must be 0 or 1 for a Bernoulli distribution.\")\n    return p if x == 1 else 1 - p\n\ntry:\n    x = int(input(\"Enter value of x (0 or 1): \"))\n    p = float(input(\"Enter probability of success p (between 0 and 1): \"))\n    \n    result = bernoulli_pdf(x, p)\n    print(f\"The probability P(X = {x}) is {result:.2f}\")\n\nexcept ValueError as e:\n    print(\"Error:\", e)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1ce10c26-c65b-4760-804d-ab3108574998",
      "cell_type": "code",
      "source": "# 4. Write a Python script to simulate a binomial distribution with n=10 and p=0.5, then plot its histogram.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nn = 10        \np = 0.5     \nsize = 10000\n\ndata = np.random.binomial(n, p, size)\n\nplt.hist(data, bins=range(n + 2), align='left', density=True, edgecolor='black')\nplt.title(f'Binomial Distribution (n={n}, p={p})')\nplt.xlabel('Number of Successes')\nplt.ylabel('Probability')\nplt.grid(True)\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "dc575c77-2a9b-46d2-a93a-afead322138c",
      "cell_type": "code",
      "source": "# 5. Creat a Poisson distribution and visualize it using Python.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nlam = 4\nsize = 10000\n\ndata = np.random.poisson(lam=lam, size=size)\n\nplt.hist(data, bins=range(0, max(data)+2), align='left', density=True, edgecolor='black')\nplt.title(f'Poisson Distribution (Œª={lam})')\nplt.xlabel('Number of Events')\nplt.ylabel('Probability')\nplt.grid(True)\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3ba011d8-1092-42be-ac64-2cd0732f5135",
      "cell_type": "code",
      "source": "# 6. Write a Python program to calculate and plot the cumulative distribution function (CDF) of a discrete uniform distribution.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\na = 1 \nb = 6\n\nx_values = np.arange(a, b + 1)\n\ncdf_values = np.arange(1, len(x_values) + 1) / len(x_values)\n\nplt.step(x_values, cdf_values, where='post', label='CDF', color='blue')\nplt.scatter(x_values, cdf_values, color='red', zorder=5)\nplt.title(f'Discrete Uniform Distribution CDF (a={a}, b={b})')\nplt.xlabel('x')\nplt.ylabel('CDF(x)')\nplt.xticks(x_values)\nplt.yticks(np.linspace(0, 1, len(x_values) + 1))\nplt.grid(True)\nplt.legend()\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5efda4e1-512f-4118-a0bd-76b809c2f47d",
      "cell_type": "code",
      "source": "# 7. Generate a continuous uniform distribution using NumPy and visualize it.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\na = 0 \nb = 10 \nsize = 10000 \n\ndata = np.random.uniform(low=a, high=b, size=size)\n\nplt.hist(data, bins=50, density=True, edgecolor='black', alpha=0.7)\nplt.title(f'Continuous Uniform Distribution (a={a}, b={b})')\nplt.xlabel('Value')\nplt.ylabel('Probability Density')\nplt.grid(True)\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9d91f2fb-2531-442a-8d43-6525a1c586d2",
      "cell_type": "code",
      "source": "# 8. Simulate data from a normal distribution and plot its histogram.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nmu = 0 \nsigma = 1 \nsize = 10000 \n\ndata = np.random.normal(loc=mu, scale=sigma, size=size)\n\nplt.hist(data, bins=50, density=True, edgecolor='black', alpha=0.7)\nplt.title(f'Normal Distribution Histogram (Œº={mu}, œÉ={sigma})')\nplt.xlabel('Value')\nplt.ylabel('Probability Density')\nplt.grid(True)\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c1ac9218-de30-4b40-b1af-c9a3c4722cc0",
      "cell_type": "code",
      "source": "# 9. Write a Python function to calculate Z-score from a dataset and plot them.\n\ndef calculate_z_scores(data):\n    mean = sum(data) / len(data)\n    std = (sum((x - mean) ** 2 for x in data) / len(data)) ** 0.5\n    return [(x - mean) / std for x in data]\n\ndata = [10, 12, 23, 23, 16, 23, 21, 16]\n\nz_scores = calculate_z_scores(data)\n\nprint(\"Data:\", data)\nprint(\"Z-scores:\")\nfor i, z in enumerate(z_scores):\n    print(f\"Value {data[i]} ‚Üí Z-score: {z:.2f}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b6e8cc63-2b76-4ec3-83cb-fa24e98d4d34",
      "cell_type": "code",
      "source": "# 10. Implement the Central Limit Theoram (CLT) using Python for a non-normal distribution.\n\nimport numpy as np\n\npopulation = np.random.exponential(scale=2.0, size=10000)\n\nsample_size = 30\nnum_samples = 100\n\nsample_means = []\n\nfor _ in range(num_samples):\n    sample = np.random.choice(population, size=sample_size)\n    sample_mean = np.mean(sample)\n    sample_means.append(sample_mean)\n\nprint(\"First 10 sample means:\")\nfor i, mean in enumerate(sample_means[:10], start=1):\n    print(f\"Sample {i}: Mean = {mean:.2f}\")\n\nprint(f\"\\nOverall average of sample means: {np.mean(sample_means):.2f}\")\nprint(f\"Standard deviation of sample means: {np.std(sample_means):.2f}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "713596b5-85a2-401a-ab76-0a0471e4ffcc",
      "cell_type": "code",
      "source": "# 15. Simulate multiple samples from a normal distribution and verify the Central Limit Theorem.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nmu = 50     \nsigma = 10 \nsample_size = 30\nnum_samples = 1000\n\nsample_means = []\n\nfor _ in range(num_samples):\n    sample = np.random.normal(loc=mu, scale=sigma, size=sample_size)\n    sample_mean = np.mean(sample)\n    sample_means.append(sample_mean)\n\nplt.hist(sample_means, bins=40, density=True, edgecolor='black', alpha=0.7)\nplt.title(f'CLT Demonstration with Normal Distribution\\nSample size = {sample_size}, Samples = {num_samples}')\nplt.xlabel('Sample Mean')\nplt.ylabel('Density')\nplt.grid(True)\nplt.axvline(np.mean(sample_means), color='red', linestyle='--', label='Mean of sample means')\nplt.legend()\nplt.show()\n\nprint(f\"Mean of sample means: {np.mean(sample_means):.2f}\")\nprint(f\"Standard deviation of sample means: {np.std(sample_means):.2f}\")\nprint(f\"Expected standard error (œÉ/‚àön): {sigma / np.sqrt(sample_size):.2f}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "id": "17abcc95-ff14-40a5-96cb-3db860269aed",
      "cell_type": "code",
      "source": "# 16. Write a Python function to calculate and plot the standard normal distribution (mean=0, std=1)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\ndef plot_standard_normal():\n\n    x = np.linspace(-4, 4, 1000)\n\n    y = norm.pdf(x, loc=0, scale=1)\n\n    plt.plot(x, y, label='Standard Normal PDF', color='blue')\n    plt.title('Standard Normal Distribution (mean=0, std=1)')\n    plt.xlabel('Z-score')\n    plt.ylabel('Probability Density')\n    plt.grid(True)\n    plt.axvline(0, color='black', linestyle='--', label='Mean (0)')\n    plt.legend()\n    plt.show()\n\nplot_standard_normal()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d2ff49bc-b6f7-40b7-9913-95a15b4231f0",
      "cell_type": "code",
      "source": "# 17. Generate random variable and calculate their corresponding probilities using the binomial distribution.\n\nimport numpy as np\nfrom scipy.stats import binom\n\nn = 10 \np = 0.5 \n\nnum_samples = 10\nrandom_vars = np.random.binomial(n=n, p=p, size=num_samples)\n\nprobabilities = binom.pmf(random_vars, n=n, p=p)\n\nprint(\"Generated Random Variables and Their Probabilities:\")\nfor i in range(num_samples):\n    print(f\"Value: {random_vars[i]} ‚Üí Probability: {probabilities[i]:.4f}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "85b1c4d1-b899-4008-97fb-0a7003e4b4ec",
      "cell_type": "code",
      "source": "# 18. Write a Python program to calculate the Z-score for a given data point and compare it to a standard normal distribution.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\ndef calculate_z_score(x, mean, std):\n    \"\"\"Calculate Z-score for a single data point.\"\"\"\n    return (x - mean) / std\n\ndata_point = 75\nmean = 70\nstd = 5\n\nz = calculate_z_score(data_point, mean, std)\nprint(f\"Data point: {data_point}\")\nprint(f\"Z-score: {z:.2f}\")\n\nx_values = np.linspace(-4, 4, 1000)\ny_values = norm.pdf(x_values, 0, 1)\n\nplt.plot(x_values, y_values, label='Standard Normal Distribution')\nplt.axvline(z, color='red', linestyle='--', label=f'Z = {z:.2f}')\nplt.fill_between(x_values, y_values, where=(x_values <= z), color='red', alpha=0.3)\nplt.title('Z-score Comparison to Standard Normal')\nplt.xlabel('Z-score')\nplt.ylabel('Probability Density')\nplt.legend()\nplt.grid(True)\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "96c5e10d-b4cd-4e32-be04-e053034fc50d",
      "cell_type": "code",
      "source": "# 19. Implement hypothesis testing using Z-statistics for a sample dataset.\n\nimport numpy as np\nfrom scipy.stats import norm\n\npopulation_mean = 70 \npopulation_std = 10  \nsample_mean = 73 \nsample_size = 50  \nalpha = 0.05   \n\nstandard_error = population_std / np.sqrt(sample_size)\nz_score = (sample_mean - population_mean) / standard_error\n\nz_critical = norm.ppf(1 - alpha)\n\nprint(f\"Z-score: {z_score:.2f}\")\nprint(f\"Critical Z-value: {z_critical:.2f}\")\n\nif z_score > z_critical:\n    print(\"Reject the null hypothesis (H‚ÇÄ). There is significant evidence.\")\nelse:\n    print(\"Fail to reject the null hypothesis (H‚ÇÄ). Not enough evidence.\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b2ee1a5a-7ed5-458a-8dd0-c3b6f8651c53",
      "cell_type": "code",
      "source": "# 20. Create a confidence interaval for a dataset using Python and interpret the result.\n\nimport numpy as np\nimport scipy.stats as stats\n\ndata = [72, 75, 70, 68, 74, 71, 69, 73, 76, 70]\n\nsample_mean = np.mean(data)\nsample_std = np.std(data, ddof=1)\nn = len(data)\nconfidence_level = 0.95\n\nalpha = 1 - confidence_level\nt_critical = stats.t.ppf(1 - alpha/2, df=n-1)\n\nmargin_of_error = t_critical * (sample_std / np.sqrt(n))\n\nci_lower = sample_mean - margin_of_error\nci_upper = sample_mean + margin_of_error\n\nprint(f\"Sample mean: {sample_mean:.2f}\")\nprint(f\"95% Confidence Interval: ({ci_lower:.2f}, {ci_upper:.2f})\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7a10cc6d-6563-4082-a784-64e21d591bbf",
      "cell_type": "code",
      "source": "# 21. Generate data from a normal distribution, then calculate and interpret the confidence interval for its mean.\n\nimport numpy as np\nfrom scipy import stats\n\nnp.random.seed(0)\nmu = 100  \nsigma = 15   \nn = 50  \n\ndata = np.random.normal(loc=mu, scale=sigma, size=n)\n\nsample_mean = np.mean(data)\nsample_std = np.std(data, ddof=1) \nse = sample_std / np.sqrt(n) \n\nconfidence = 0.95\nalpha = 1 - confidence\nt_critical = stats.t.ppf(1 - alpha/2, df=n-1)\n\nmargin_of_error = t_critical * se\nci_lower = sample_mean - margin_of_error\nci_upper = sample_mean + margin_of_error\n\nprint(f\"Sample Mean: {sample_mean:.2f}\")\nprint(f\"95% Confidence Interval for the Mean: ({ci_lower:.2f}, {ci_upper:.2f})\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d2bc1272-c8ac-46fd-aeb3-251f62e7e64c",
      "cell_type": "code",
      "source": "# 22. Write a Python script to calculate and visualize the probability density function (PDF) of a normal distribution.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nmean = 0  \nstd_dev = 1 \n\nx = np.linspace(mean - 4*std_dev, mean + 4*std_dev, 1000)\n\npdf = norm.pdf(x, loc=mean, scale=std_dev)\n\nplt.figure(figsize=(8, 5))\nplt.plot(x, pdf, label=f'Normal PDF\\n(mean={mean}, std={std_dev})', color='blue')\nplt.title('Probability Density Function of Normal Distribution')\nplt.xlabel('x')\nplt.ylabel('Probability Density')\nplt.grid(True)\nplt.legend()\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1ac0bb73-1b87-4702-8175-c9cf43c79c92",
      "cell_type": "code",
      "source": "# 23. Use Python to calculate and interpret the cumulative distribution and calculate its expected value.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nnp.random.seed(42)\nmean = 50\nstd_dev = 10\nn = 1000\ndata = np.random.normal(loc=mean, scale=std_dev, size=n)\n\nx = np.sort(data)\ny = np.arange(1, n + 1) / n\n\nplt.figure(figsize=(8, 5))\nplt.plot(x, y, marker='.', linestyle='none', label='Empirical CDF')\nplt.title('Empirical Cumulative Distribution Function (CDF)')\nplt.xlabel('Value')\nplt.ylabel('Cumulative Probability')\nplt.grid(True)\nplt.legend()\nplt.show()\n\nexpected_value = np.mean(data)\nprint(f\"Expected value (sample mean): {expected_value:.2f}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3f2219d4-5961-4207-b67a-d340022e9500",
      "cell_type": "code",
      "source": "# 24. Simulate a radom variable using a continuous uniform distribution and calculate its expected value.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\na = 10 \nb = 30 \nn = 1000 \n\ndata = np.random.uniform(low=a, high=b, size=n)\n\nsample_mean = np.mean(data)\ntheoretical_mean = (a + b) / 2\n\nplt.figure(figsize=(8, 5))\nplt.hist(data, bins=30, density=True, edgecolor='black', alpha=0.7)\nplt.axvline(sample_mean, color='red', linestyle='--', label=f'Sample Mean: {sample_mean:.2f}')\nplt.axvline(theoretical_mean, color='green', linestyle='--', label=f'Theoretical Mean: {theoretical_mean:.2f}')\nplt.title('Histogram of Continuous Uniform Distribution')\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nprint(f\"Sample Expected Value (Mean): {sample_mean:.2f}\")\nprint(f\"Theoretical Expected Value: {theoretical_mean:.2f}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2621758c-57e4-4f51-ac5a-1224af6e9a4a",
      "cell_type": "code",
      "source": "# 25. Write a Pyton program to compare the standard deviations of two datasets and visualize the difference.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)\ndata1 = np.random.normal(loc=50, scale=5, size=1000)  \ndata2 = np.random.normal(loc=50, scale=15, size=1000) \n\nstd1 = np.std(data1, ddof=1)\nstd2 = np.std(data2, ddof=1)\n\nprint(f\"Standard Deviation of Dataset 1: {std1:.2f}\")\nprint(f\"Standard Deviation of Dataset 2: {std2:.2f}\")\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.hist(data1, bins=30, alpha=0.6, label='Dataset 1', color='blue', edgecolor='black')\nplt.hist(data2, bins=30, alpha=0.6, label='Dataset 2', color='orange', edgecolor='black')\nplt.title('Histogram Comparison')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.legend()\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.box\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5d86dc3f-c6df-4f4d-bc01-877654ea82a5",
      "cell_type": "code",
      "source": "# 26. Calculate the range and interquartile range (IQR) of a dataset generated from a normal distribution.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\ndata = np.random.normal(loc=100, scale=15, size=1000)\n\ndata_range = np.max(data) - np.min(data)\n\nq1 = np.percentile(data, 25)\nq3 = np.percentile(data, 75)\niqr = q3 - q1\n\nprint(f\"Range: {data_range:.2f}\")\nprint(f\"IQR (Interquartile Range): {iqr:.2f}\")\n\nplt.figure(figsize=(6, 4))\nplt.boxplot(data, vert=False)\nplt.title('Boxplot of Normally Distributed Data')\nplt.xlabel('Value')\nplt.grid(True)\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "33e96e54-dce6-40fb-a0a7-58c10fa3e7a0",
      "cell_type": "code",
      "source": "# 27. Implement Z-score normalization on a dataset and visulaize its transformation.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(1)\ndata = np.random.normal(loc=50, scale=10, size=1000)\n\nmean = np.mean(data)\nstd = np.std(data)\nz_scores = (data - mean) / std\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.hist(data, bins=30, color='skyblue', edgecolor='black')\nplt.title('Original Data')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.axvline(mean, color='red', linestyle='--', label=f'Mean = {mean:.2f}')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.hist(z_scores, bins=30, color='lightgreen', edgecolor='black')\nplt.title('Z-Score Normalized Data')\nplt.xlabel('Z-score')\nplt.ylabel('Frequency')\nplt.axvline(0, color='red', linestyle='--', label='Mean = 0')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Original Mean: {mean:.2f}, Original Std Dev: {std:.2f}\")\nprint(f\"Z-Score Mean: {np.mean(z_scores):.2f}, Z-Score Std Dev: {np.std(z_scores):.2f}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "546cb71a-ae75-4e11-8133-21602d8f25c4",
      "cell_type": "code",
      "source": "# 28. Write a Python function to calculate the skewness and kurtosis of a dataset generated from a normal distribution.\n\nimport numpy as np\nfrom scipy.stats import skew, kurtosis\n\ndata = np.random.normal(0, 1, 1000)\n\nprint(\"Skewness:\", skew(data))\nprint(\"Kurtosis:\", kurtosis(data)) \n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}